{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1loDGSKIBdbU"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from scipy.special import rel_entr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2kU6J8vzGRi"
   },
   "outputs": [],
   "source": [
    "def solve_Q_new(P: np.ndarray):\n",
    "  '''\n",
    "  Compute optimal Q given 3d array P \n",
    "  with dimensions coressponding to x1, x2, and y respectively\n",
    "  '''\n",
    "  Py = P.sum(axis=0).sum(axis=0)\n",
    "  Px1 = P.sum(axis=1).sum(axis=1)\n",
    "  Px2 = P.sum(axis=0).sum(axis=1)\n",
    "  Px2y = P.sum(axis=0)\n",
    "  Px1y = P.sum(axis=1)\n",
    "  Px1y_given_x2 = P/P.sum(axis=(0,2),keepdims=True)\n",
    " \n",
    "  Q = [cp.Variable((P.shape[0], P.shape[1]), nonneg=True) for i in range(P.shape[2])]\n",
    "  Q_x1x2 = [cp.Variable((P.shape[0], P.shape[1]), nonneg=True) for i in range(P.shape[2])]\n",
    "\n",
    "  # Constraints that conditional distributions sum to 1\n",
    "  sum_to_one_Q = cp.sum([cp.sum(q) for q in Q]) == 1\n",
    "\n",
    "  # Brute force constraints # \n",
    "  # [A]: p(x1, y) == q(x1, y) \n",
    "  # [B]: p(x2, y) == q(x2, y)\n",
    "\n",
    "  # Adding [A] constraints\n",
    "  A_cstrs = []\n",
    "  for x1 in range(P.shape[0]):\n",
    "      for y in range(P.shape[2]):\n",
    "        vars = []\n",
    "        for x2 in range(P.shape[1]):\n",
    "          vars.append(Q[y][x1, x2])\n",
    "        A_cstrs.append(cp.sum(vars) == Px1y[x1,y])\n",
    "  \n",
    "  # Adding [B] constraints\n",
    "  B_cstrs = []\n",
    "  for x2 in range(P.shape[1]):\n",
    "      for y in range(P.shape[2]):\n",
    "        vars = []\n",
    "        for x1 in range(P.shape[0]):\n",
    "          vars.append(Q[y][x1, x2])\n",
    "        B_cstrs.append(cp.sum(vars) == Px2y[x2,y])\n",
    "\n",
    "  # KL divergence\n",
    "  Q_pdt_dist_cstrs = [cp.sum(Q) / P.shape[2] == Q_x1x2[i] for i in range(P.shape[2])]\n",
    "\n",
    "\n",
    "  # objective\n",
    "  obj = cp.sum([cp.sum(cp.rel_entr(Q[i], Q_x1x2[i])) for i in range(P.shape[2])])\n",
    "  # print(obj.shape)\n",
    "  all_constrs = [sum_to_one_Q] + A_cstrs + B_cstrs + Q_pdt_dist_cstrs\n",
    "  prob = cp.Problem(cp.Minimize(obj), all_constrs)\n",
    "  prob.solve(verbose=True, max_iters=50000)\n",
    "\n",
    "  # print(prob.status)\n",
    "  # print(prob.value)\n",
    "  # for j in range(P.shape[1]):\n",
    "  #  print(Q[j].value)\n",
    "\n",
    "  return np.stack([q.value for q in Q],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5K-tL7jd6kB"
   },
   "outputs": [],
   "source": [
    "def gen_binary_data(num_data):\n",
    "  # 00  0\n",
    "  # 01  0\n",
    "  # 10  0\n",
    "  # 11  1\n",
    "\n",
    "  x1 = np.random.randint(0, 2, (num_data, 1))\n",
    "  x2 = np.random.randint(0, 2, (num_data, 1))\n",
    "  data = {\n",
    "      'and': (x1, x2, 1 * np.logical_and(x1, x2)),\n",
    "      'or': (x1, x2, 1 * np.logical_or(x1, x2)),\n",
    "      'xor': (x1, x2, 1 * np.logical_xor(x1, x2)),\n",
    "      'unique1': (x1, x2, x1),\n",
    "      'redundant': (x1, x1, x1),\n",
    "      'redundant_and_unique1': (np.concatenate([x1, x2], axis=1), x2, 1 * np.logical_and(x1, x2)),\n",
    "      'redundant_or_unique1': (np.concatenate([x1, x2], axis=1), x2, 1 * np.logical_or(x1, x2)),\n",
    "      'redundant_xor_unique1': (np.concatenate([x1, x2], axis=1), x2, 1 * np.logical_xor(x1, x2)),\n",
    "  }\n",
    "  return data\n",
    "\n",
    "def convert_data_to_distribution(x1: np.ndarray, x2: np.ndarray, y: np.ndarray):\n",
    "  assert x1.size == x2.size\n",
    "  assert x1.size == y.size\n",
    "\n",
    "  numel = x1.size\n",
    "  \n",
    "  x1_discrete, x1_raw_to_discrete = extract_categorical_from_data(x1.squeeze())\n",
    "  x2_discrete, x2_raw_to_discrete = extract_categorical_from_data(x2.squeeze())\n",
    "  y_discrete, y_raw_to_discrete = extract_categorical_from_data(y.squeeze())\n",
    "\n",
    "  joint_distribution = np.zeros((len(x1_raw_to_discrete), len(x2_raw_to_discrete), len(y_raw_to_discrete)))\n",
    "  for i in range(numel):\n",
    "    joint_distribution[x1_discrete[i], x2_discrete[i], y_discrete[i]] += 1\n",
    "  joint_distribution /= np.sum(joint_distribution)\n",
    "\n",
    "  return joint_distribution, (x1_raw_to_discrete, x2_raw_to_discrete, y_raw_to_discrete)\n",
    "\n",
    "def extract_categorical_from_data(x):\n",
    "  supp = set(x)\n",
    "  raw_to_discrete = dict()\n",
    "  for i in supp:\n",
    "    raw_to_discrete[i] = len(raw_to_discrete)\n",
    "  discrete_data = [raw_to_discrete[x_] for x_ in x]\n",
    "\n",
    "  return discrete_data, raw_to_discrete \n",
    "\n",
    "def MI(P: np.ndarray):\n",
    "  ''' P has 2 dimensions '''\n",
    "  margin_1 = P.sum(axis=1)\n",
    "  margin_2 = P.sum(axis=0)\n",
    "  outer = np.outer(margin_1, margin_2)\n",
    "\n",
    "  return np.sum(rel_entr(P, outer))\n",
    "  # return np.sum(P * np.log(P/outer))\n",
    "\n",
    "def CoI(P:np.ndarray):\n",
    "  ''' P has 3 dimensions, in order X1, X2, Y '''\n",
    "  # MI(Y; X1)\n",
    "  A = P.sum(axis=1)\n",
    "\n",
    "  # MI(Y; X2)\n",
    "  B = P.sum(axis=0)\n",
    "\n",
    "  # MI(Y; (X1, X2))\n",
    "  C = P.transpose([2, 0, 1]).reshape((P.shape[2], P.shape[0]*P.shape[1]))\n",
    "\n",
    "  return MI(A) + MI(B) - MI(C)\n",
    "\n",
    "def CI(P, Q):\n",
    "  assert P.shape == Q.shape\n",
    "  P_ = P.transpose([2, 0, 1]).reshape((P.shape[2], P.shape[0]*P.shape[1]))\n",
    "  Q_ = Q.transpose([2, 0, 1]).reshape((Q.shape[2], Q.shape[0]*Q.shape[1]))\n",
    "  return MI(P_) - MI(Q_)\n",
    "\n",
    "def UI(P, cond_id=0):\n",
    "  ''' P has 3 dimensions, in order X1, X2, Y \n",
    "  We condition on X1 if cond_id = 0, if 1, then X2.\n",
    "  '''\n",
    "  P_ = np.copy(P)\n",
    "  sum = 0.\n",
    "\n",
    "  if cond_id == 0:\n",
    "    J= P.sum(axis=(1,2)) # marginal of x1\n",
    "    for i in range(P.shape[0]):\n",
    "      sum += MI(P[i,:,:]/P[i,:,:].sum()) * J[i]\n",
    "  elif cond_id == 1:\n",
    "    J= P.sum(axis=(0,2)) # marginal of x1\n",
    "    for i in range(P.shape[1]):\n",
    "      sum += MI(P[:,i,:]/P[:,i,:].sum()) * J[i]\n",
    "  else:\n",
    "    assert False\n",
    "\n",
    "  return sum\n",
    "\n",
    "def test(P):\n",
    "  Q = solve_Q_new(P)\n",
    "  redundancy = CoI(Q)\n",
    "  print('Redundancy', redundancy)\n",
    "  unique_1 = UI(Q, cond_id=1)\n",
    "  print('Unique', unique_1)\n",
    "  unique_2 = UI(Q, cond_id=0)\n",
    "  print('Unique', unique_2)\n",
    "  synergy = CI(P, Q)\n",
    "  print('Synergy', synergy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1aRc6d2EVKa"
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xrvx5ISEUly",
    "outputId": "2bd9dee0-7f7e-40e1-febe-e32287ff37e0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jla3uW64BCAb",
    "outputId": "d6375d56-c5c2-42b7-c33c-ac79f8d16e45"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from my_helper_functions import my_ensemble_test, mytest, AdditiveEnsemble\n",
    "from datasets.affect.get_data import get_dataloader  # noqa\n",
    "\n",
    "traindata, validdata, testdata = get_dataloader(\n",
    "    'humor.pkl', robust_test=False, max_pad=True, data_type='humor', max_seq_len=50, train_shuffle=False)\n",
    "\n",
    "ensemble = torch.load(f'humor_ensemble_12.pt', map_location=torch.device('cpu'))\n",
    "my_ensemble_test(ensemble, testdata, f'humor_ensemble_test_12_pred.pkl', 'humor', \n",
    "         no_robust=True, criterion=torch.nn.L1Loss(), task='posneg-classification')\n",
    "my_ensemble_test(ensemble, traindata, f'humor_ensemble_train_12_pred.pkl', 'humor', \n",
    "         no_robust=True, criterion=torch.nn.L1Loss(), task='posneg-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m41DuWhX8Aec",
    "outputId": "be60f99f-4290-450b-b70a-b9ae8ec7dc70"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "\n",
    "for model in ['lower', 'mi']:\n",
    "  with open(f'sarcasm_{model}_01_pred.pkl', 'rb') as f:\n",
    "      x = CPU_Unpickler(f).load()\n",
    "  # print(x)\n",
    "  # pred = []\n",
    "  # for p in x:\n",
    "  #   pred.extend(list(p))\n",
    "  # pred = [int(p) for p in pred]\n",
    "  # print(pred)\n",
    "  pred = x\n",
    "  # print(pred)\n",
    "\n",
    "  with open('sarcasm_clusters.pkl', 'rb') as f:\n",
    "      clusters = pickle.load(f)\n",
    "  # replace_neg = np.vectorize (lambda x: 0 if x < 0 else x)\n",
    "  # pred = replace_neg(pred)\n",
    "  # print('pred', pred, len(pred))\n",
    "\n",
    "  # print(clusters.keys())\n",
    "  # print(clusters['test'].keys())\n",
    "  labels = clusters['test']['labels']\n",
    "  # print(labels.shape)\n",
    "  # print('labels', labels, len(labels))\n",
    "\n",
    "  import numpy as np\n",
    "  # pred = np.array(pred).astype(bool)\n",
    "  # humor\n",
    "  print(model)\n",
    "  print(np.sum(np.equal(np.array(pred).astype(int), labels))/len(labels))\n",
    "  # mosei\n",
    "  # print(np.sum(np.equal(np.array(pred).astype(int).flatten(), flags.flatten()))/len(flags.flatten()))\n",
    "  # import pickle\n",
    "  # with open('enrico_clusters.pkl', 'rb') as f:\n",
    "  #     clusters = pickle.load(f)\n",
    "  c1 = clusters['test']['vision']\n",
    "  c2 = clusters['test']['audio']\n",
    "  labels = clusters['test']['labels']\n",
    "  P, maps = convert_data_to_distribution(np.array(c1), np.array(c2), np.array(pred))\n",
    "  test(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3dxYhjs9-Hv"
   },
   "source": [
    "### Using data distribution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMMeTobG97J9"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mosei_align_12_pred.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "print(x)\n",
    "pred = []\n",
    "for p in x:\n",
    "  pred.extend(list(p))\n",
    "pred = [int(p) for p in pred]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxcSTyk5PkGF"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWP_kEeiPmzp"
   },
   "outputs": [],
   "source": [
    "def clustering(X, pca=False, n_clusters=30):\n",
    "  X = np.nan_to_num(X)\n",
    "  if len(X.shape) > 2:\n",
    "    X = X.reshape(X.shape[0],-1)\n",
    "  if pca:\n",
    "    # print(np.any(np.isnan(X)), np.all(np.isfinite(X)))\n",
    "    X = normalize(X)\n",
    "    X = PCA(n_components=5).fit_transform(X)\n",
    "  kmeans = KMeans(n_clusters=n_clusters).fit(X)\n",
    "  return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-95s-aO3pkc",
    "outputId": "e7f4848a-d037-45b6-a3fa-27dea1462fb1"
   },
   "outputs": [],
   "source": [
    "data_dir = 'sarcasm.pkl'\n",
    "dataset = pd.read_pickle(data_dir)\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2hxKTbKTgDx"
   },
   "outputs": [],
   "source": [
    "data_cluster = dict()\n",
    "for split in dataset:\n",
    "  if split != 'test':\n",
    "    continue\n",
    "  data_cluster[split] = dict()\n",
    "  data = dataset[split]\n",
    "  data_cluster[split]['vision'] = clustering(data['vision'], pca=True).reshape(-1,1)\n",
    "  data_cluster[split]['audio'] = clustering(data['audio'], pca=True).reshape(-1,1)\n",
    "  data_cluster[split]['text'] = clustering(data['text'], pca=True).reshape(-1,1)\n",
    "  data_cluster[split]['labels'] = data['labels']\n",
    "  data_cluster[split]['id'] = data['id']\n",
    "with open('sarcasm_clusters.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cluster, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGmfVgDi5TaY",
    "outputId": "bd5ec4e6-642d-4e4c-d7b3-6f4073c8e847"
   },
   "outputs": [],
   "source": [
    "len(pred)\n",
    "replace_neg = np.vectorize (lambda x: 0 if x < 0 else x)\n",
    "pred = replace_neg(pred)\n",
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rbTgAQe-fK9"
   },
   "outputs": [],
   "source": [
    "clusters = data_cluster\n",
    "labels = clusters['test']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR40lkvJBeOv"
   },
   "outputs": [],
   "source": [
    "l = labels[:,:,0]\n",
    "l.shape\n",
    "flags = []\n",
    "def _get_class(flag, data_type='mosei'):\n",
    "    if data_type in ['mosi', 'mosei', 'sarcasm']:\n",
    "        if flag > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return [flag]\n",
    "for ll in l:\n",
    "  flags.append((_get_class(ll)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6CcMxTl2jPq"
   },
   "outputs": [],
   "source": [
    "with open('humor_lf_0_features_clusters.pkl', 'rb') as f:\n",
    "  fc0 = pickle.load(f)\n",
    "with open('humor_lf_1_features_clusters.pkl', 'rb') as f:\n",
    "  fc1 = pickle.load(f)\n",
    "\n",
    "with open('humor_clusters.pkl', 'rb') as f:\n",
    "    clusters = pickle.load(f)\n",
    "# replace_neg = np.vectorize (lambda x: 0 if x < 0 else x)\n",
    "# pred = replace_neg(pred)\n",
    "# print(pred, len(pred))\n",
    "\n",
    "print(clusters.keys())\n",
    "print(clusters['test'].keys())\n",
    "labels = clusters['test']['labels']\n",
    "print(labels.shape)\n",
    "\n",
    "P, maps = convert_data_to_distribution(np.array(fc0), np.array(fc1), np.array(labels))\n",
    "test(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ftfvepkqqos",
    "outputId": "d3eaab68-fe81-4cec-c777-aa8198e216a2"
   },
   "outputs": [],
   "source": [
    "clusters['test'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fA7YO6ef-Vtq"
   },
   "outputs": [],
   "source": [
    "c1 = clusters['test']['audio']\n",
    "c2 = clusters['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2kQhrQV-vIK",
    "outputId": "99d084c4-9854-459c-923b-6cd80d2cd7ff"
   },
   "outputs": [],
   "source": [
    "P, maps = convert_data_to_distribution(np.array(c1), np.array(c2), np.array(labels))\n",
    "test(P)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
