{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbtAQJ8jm3yq",
    "outputId": "bb41faa1-1085-4d7c-e488-46aefe00a9d3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZZtDrlpq8qw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Datasets\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "  def __init__(self, data, labels):\n",
    "    self.data = data\n",
    "    self.labels = labels\n",
    "    self.num_modalities = len(self.data)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return tuple([self.data[i][idx] for i in range(self.num_modalities)] + [self.labels[idx]])\n",
    "\n",
    "# Models\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def sinkhorn_probs(matrix, x1_probs, x2_probs):\n",
    "    matrix = matrix / (torch.sum(matrix, dim=0, keepdim=True) + 1e-8) * x2_probs[None]\n",
    "    sum = torch.sum(matrix, dim=1)\n",
    "    if torch.allclose(sum, x1_probs, rtol=0, atol=0.01):\n",
    "        return matrix, True\n",
    "    matrix = matrix / (torch.sum(matrix, dim=1, keepdim=True) + 1e-8) * x1_probs[:, None]\n",
    "    sum = torch.sum(matrix, dim=0)\n",
    "    if torch.allclose(sum, x2_probs, rtol=0, atol=0.01):\n",
    "        return matrix, True\n",
    "    return matrix, False\n",
    "\n",
    "def mlp(dim, hidden_dim, output_dim, layers, activation):\n",
    "    activation = {\n",
    "        'relu': nn.ReLU,\n",
    "        'tanh': nn.Tanh,\n",
    "    }[activation]\n",
    "\n",
    "    seq = [nn.Linear(dim, hidden_dim), activation()]\n",
    "    for _ in range(layers):\n",
    "        seq += [nn.Linear(hidden_dim, hidden_dim), activation()]\n",
    "    seq += [nn.Linear(hidden_dim, output_dim)]\n",
    "\n",
    "    return nn.Sequential(*seq)\n",
    "\n",
    "def simple_discrim(xs, y, num_labels):\n",
    "    shape = [x.size(1) for x in xs] + [num_labels]\n",
    "    p = torch.ones(*shape) * 1e-8\n",
    "    for i in range(len(y)):\n",
    "        p[tuple([torch.argmax(x[i]).item() for x in xs] + [y[i].item()])] += 1\n",
    "    p /= torch.sum(p)\n",
    "    p = p.cuda()\n",
    "    \n",
    "    def f(*x):\n",
    "        x = [torch.argmax(xx, dim=1) for xx in x]\n",
    "        return torch.log(p[tuple(x)])\n",
    "\n",
    "    return f\n",
    "\n",
    "class Discrim(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim, num_labels, layers, activation):\n",
    "        super().__init__()\n",
    "        self.mlp = mlp(x_dim, hidden_dim, num_labels, layers, activation)\n",
    "    def forward(self, *x):\n",
    "        x = torch.cat(x, dim=-1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class CEAlignment(nn.Module):\n",
    "    def __init__(self, x1_dim, x2_dim, hidden_dim, embed_dim, num_labels, layers, activation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.mlp1 = mlp(x1_dim, hidden_dim, embed_dim * num_labels, layers, activation)\n",
    "        self.mlp2 = mlp(x2_dim, hidden_dim, embed_dim * num_labels, layers, activation)\n",
    "\n",
    "    def forward(self, x1, x2, x1_probs, x2_probs):\n",
    "        x1_input = x1\n",
    "        x2_input = x2\n",
    "\n",
    "        q_x1 = self.mlp1(x1).unflatten(1, (self.num_labels, -1))\n",
    "        q_x2 = self.mlp2(x2).unflatten(1, (self.num_labels, -1))\n",
    "\n",
    "        q_x1 = (q_x1 - torch.mean(q_x1, dim=2, keepdim=True)) / torch.sqrt(torch.var(q_x1, dim=2, keepdim=True) + 1e-8)\n",
    "        q_x2 = (q_x2 - torch.mean(q_x2, dim=2, keepdim=True)) / torch.sqrt(torch.var(q_x2, dim=2, keepdim=True) + 1e-8)\n",
    "\n",
    "        # print(q_x1)\n",
    "\n",
    "        align = torch.einsum('ahx, bhx -> abh', q_x1, q_x2) / math.sqrt(q_x1.size(-1))\n",
    "        # print(q_x1[0])\n",
    "        # print(q_x2[0])\n",
    "        # print(q_x1[0] * q_x2[0])\n",
    "        # print(torch.sum(q_x1[0] * q_x2[0]))\n",
    "        # print(align)\n",
    "        align_logits = align\n",
    "        align = torch.exp(align)\n",
    "        # print(x1_input[:10])\n",
    "        # print(x2_input[:10])\n",
    "        # print(x1_input[:, 0])\n",
    "        # print(x2_input[:, 0][None])\n",
    "        # print(x1_input[:, 0, None] == x2_input[:, 0][None])\n",
    "        # align = (x1_input[:, 0, None] == x2_input[:, 0][None]) + align - align.detach()\n",
    "\n",
    "        # print(align[:10, :10])\n",
    "\n",
    "        normalized = []\n",
    "        for i in range(align.size(-1)):\n",
    "            current = align[..., i]\n",
    "            for j in range(500): # TODO\n",
    "                current, stop = sinkhorn_probs(current, x1_probs[:, i], x2_probs[:, i])\n",
    "                if stop:\n",
    "                    break\n",
    "            normalized.append(current)\n",
    "        normalized = torch.stack(normalized, dim=-1)\n",
    "\n",
    "        if torch.any(torch.isnan(normalized)):\n",
    "            print(align_logits)\n",
    "            print(align)\n",
    "            print(normalized)\n",
    "            raise Exception('nan')\n",
    "\n",
    "        return normalized\n",
    "\n",
    "class CEAlignmentInformation(nn.Module):\n",
    "    def __init__(self, x1_dim, x2_dim, hidden_dim, embed_dim, num_labels,\n",
    "                 layers, activation, discrim_1, discrim_2, discrim_12, p_y):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.align = CEAlignment(x1_dim, x2_dim, hidden_dim, embed_dim, num_labels, layers, activation)\n",
    "        self.discrim_1 = discrim_1\n",
    "        if isinstance(self.discrim_1, nn.Module):\n",
    "            self.discrim_1.eval()\n",
    "        self.discrim_2 = discrim_2\n",
    "        if isinstance(self.discrim_2, nn.Module):\n",
    "            self.discrim_2.eval()\n",
    "        self.discrim_12 = discrim_12\n",
    "        if isinstance(self.discrim_12, nn.Module):\n",
    "            self.discrim_12.eval()\n",
    "        self.register_buffer('p_y', p_y)\n",
    "        # self.critic_1y = SeparableCritic(x1_dim, y_dim, hidden_dim, embed_dim, layers, activation)\n",
    "        # self.critic_2y = SeparableCritic(x2_dim, y_dim, hidden_dim, embed_dim, layers, activation)\n",
    "        # self.critic_12y = SeparableCritic(x1_dim + x2_dim, y_dim, hidden_dim, embed_dim, layers, activation)\n",
    "\n",
    "    def align_parameters(self):\n",
    "        return list(self.align.parameters())\n",
    "\n",
    "    def forward(self, x1, x2, y):\n",
    "        # print('forward', x1.shape, x2.shape, y.shape)\n",
    "        with torch.no_grad():\n",
    "            a = self.discrim_1([x1])\n",
    "            # print('a', a.shape)\n",
    "            b = self.discrim_2([x2])\n",
    "            # print('b', b.shape)\n",
    "            p_y_x1 = nn.Softmax(dim=-1)(a)\n",
    "            p_y_x2 = nn.Softmax(dim=-1)(b)\n",
    "        align = self.align(torch.flatten(x1, 1, -1), torch.flatten(x2, 1, -1), p_y_x1, p_y_x2)\n",
    "        # print(p_y_x2)\n",
    "        # print(self.p_y)\n",
    "        # print(y.squeeze(-1))\n",
    "        y = nn.functional.one_hot(y.squeeze(-1).long(), num_classes=self.num_labels)\n",
    "        self.p_y[self.p_y == 0] += 1e-8\n",
    "        self.p_y[self.p_y == 1] -= 1e-8\n",
    "\n",
    "        # sample method: P(X1)\n",
    "        # coeff: P(Y | X1) Q(X2 | X1, Y)\n",
    "        # log term: log Q(X2 | X1, Y) - logsum_Y' Q(X2 | X1, Y') Q(Y' | X1)\n",
    "\n",
    "        q_x2_x1y = align / (torch.sum(align, dim=1, keepdim=True) + 1e-8)\n",
    "        # print(torch.cat([1 - y, y], dim=-1).shape)\n",
    "        log_term = torch.log(q_x2_x1y + 1e-8) - torch.log(torch.einsum('aby, ay -> ab', q_x2_x1y, p_y_x1) + 1e-8)[:, :, None]\n",
    "        # print(q_x2_x1y)\n",
    "        # print(log_term)\n",
    "        # That's all we need for optimization purposes\n",
    "        loss = torch.mean(torch.sum(torch.sum(p_y_x1[:, None, :] * q_x2_x1y * log_term, dim=-1), dim=-1))\n",
    "        # Now, we calculate the MI terms\n",
    "        p_y_x1_sampled = torch.sum(p_y_x1 * y, dim=-1)\n",
    "        p_y_x2_sampled = torch.sum(p_y_x2 * y, dim=-1)\n",
    "        # print(p_y_x2_sampled)\n",
    "        with torch.no_grad():\n",
    "            p_y_x1x2 = nn.Softmax(dim=-1)(self.discrim_12([x1, x2]))\n",
    "        p_y_x1x2_sampled = torch.sum(p_y_x1x2 * y, dim=-1)\n",
    "        p_y_sampled = torch.sum(self.p_y[None] * y, dim=-1)\n",
    "\n",
    "        p1 = p_y_x1.detach().clone()\n",
    "        p1[p1 == 0] += 1e-8\n",
    "        log_p_y_x1 = torch.log(p1)\n",
    "        # log_p_y_x1[log_p_y_x1 == float(\"-Inf\")] += 1e-8\n",
    "        p2 = p_y_x2.detach().clone()\n",
    "        p2[p2 == 0] += 1e-8\n",
    "        log_p_y_x2 = torch.log(p2)\n",
    "        # log_p_y_x2[log_p_y_x2 == float(\"-Inf\")] += 1e-8\n",
    "        p12 = p_y_x1x2.detach().clone()\n",
    "        p12[p12 == 0] += 1e-8\n",
    "        log_p_y_x1x2 = torch.log(p12)\n",
    "        # log_p_y_x1x2[log_p_y_x1x2 == float(\"-Inf\")] += 1e-8\n",
    "\n",
    "        # mi_y_x1 = torch.mean(torch.log(p_y_x1_sampled) - torch.log(p_y_sampled))\n",
    "        mi_y_x1 = torch.mean(torch.sum(p_y_x1 * (log_p_y_x1 - torch.log(self.p_y)[None]), dim=-1))\n",
    "        # mi_y_x2 = torch.mean(torch.log(p_y_x2_sampled) - torch.log(p_y_sampled))\n",
    "        mi_y_x2 = torch.mean(torch.sum(p_y_x2 * (log_p_y_x2 - torch.log(self.p_y)[None]), dim=-1))\n",
    "        # mi_y_x1x2 = torch.mean(torch.log(p_y_x1x2_sampled) - torch.log(p_y_sampled))\n",
    "        mi_y_x1x2 = torch.mean(torch.sum(p_y_x1x2 * (log_p_y_x1x2 - torch.log(self.p_y)[None, None]), dim=-1))\n",
    "        mi_q_y_x1x2 = p_y_x1[:, None, :] * q_x2_x1y * (log_term + torch.log(p_y_x1 + 1e-8)[:, None, :] - torch.log(self.p_y + 1e-8)[None, None, :])\n",
    "        '''\n",
    "        if not self.training:\n",
    "            print(p_y_x1)\n",
    "            print(q_x2_x1y)\n",
    "            print(log_term)\n",
    "            print(torch.log(p_y_x1))\n",
    "            print(torch.log(self.p_y))\n",
    "            print(log_term + torch.log(p_y_x1)[:, None, :] - torch.log(self.p_y)[None, None, :])\n",
    "        '''\n",
    "        mi_q_y_x1x2 = torch.sum(torch.sum(mi_q_y_x1x2, dim=-1), dim=-1) # anchored by x1 -- take mean to get MI\n",
    "        mi_q_y_x1x2 = torch.mean(mi_q_y_x1x2)\n",
    "\n",
    "        '''\n",
    "        if not self.training:\n",
    "            print(torch.stack([mi_y_x1, mi_y_x2, mi_y_x1x2, mi_q_y_x1x2]))\n",
    "        '''\n",
    "        # print('   m', torch.stack([mi_y_x1, mi_y_x2, mi_y_x1x2, mi_q_y_x1x2]))\n",
    "\n",
    "        redundancy = mi_y_x1 + mi_y_x2 - mi_q_y_x1x2\n",
    "        unique1 = mi_q_y_x1x2 - mi_y_x2\n",
    "        unique2 = mi_q_y_x1x2 - mi_y_x1\n",
    "        synergy = mi_y_x1x2 - mi_q_y_x1x2\n",
    "\n",
    "        # print('   r', torch.stack([redundancy, unique1, unique2, synergy]))\n",
    "\n",
    "        return loss, torch.stack([redundancy, unique1, unique2, synergy], dim=0), align\n",
    "\n",
    "# Training Loops\n",
    "from tqdm import tqdm\n",
    "def train_discrim(model, train_loader, optimizer, data_type, num_epoch=40):\n",
    "    for _iter in range(num_epoch):\n",
    "        print(_iter)\n",
    "        for i_batch, data_batch in enumerate(tqdm(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = []\n",
    "            for j in range(len(data_type)):\n",
    "                xs = [data_batch[data_type[j][i] - 1] for i in range(len(data_type[j]))]\n",
    "                x_batch = torch.cat(xs, dim=1).cuda()\n",
    "                if j != len(data_type) - 1:\n",
    "                    x_batch = x_batch.float()\n",
    "                inputs.append(x_batch)\n",
    "            y = inputs[-1]\n",
    "            inputs = inputs[:-1]\n",
    "\n",
    "            logits = model(*inputs)\n",
    "            loss = nn.CrossEntropyLoss()(logits, y.squeeze(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (_iter + 1) % 20 == 0 and i_batch % 1024 == 0:\n",
    "                print('iter: ', _iter, ' i_batch: ', i_batch, ' loss: ', loss.item())\n",
    "\n",
    "def eval_discrim(model, test_loader, data_type):\n",
    "    losses = []\n",
    "    for i_batch, data_batch in enumerate(test_loader):\n",
    "        inputs = []\n",
    "        for j in range(len(data_type)):\n",
    "            xs = [data_batch[data_type[j][i] - 1] for i in range(len(data_type[j]))]\n",
    "            x_batch = torch.cat(xs, dim=1).cuda()\n",
    "            if j != len(data_type) - 1:\n",
    "                x_batch = x_batch.float()\n",
    "            inputs.append(x_batch)\n",
    "        y = inputs[-1]\n",
    "        inputs = inputs[:-1]\n",
    "\n",
    "        logits = model(*inputs)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y.squeeze(-1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if i_batch % 1024 == 0:\n",
    "            print('i_batch: ', i_batch, ' loss: ', loss.item())\n",
    "    print('Eval loss:', sum(losses) / len(losses))\n",
    "\n",
    "def train_ce_alignment(model, train_loader, opt_align, data_type, num_epoch=10):\n",
    "    for _iter in range(num_epoch):\n",
    "        print(_iter)\n",
    "        for i_batch, data_batch in enumerate(tqdm(train_loader)):\n",
    "            opt_align.zero_grad()\n",
    "\n",
    "            x1s = [data_batch[data_type[0][i] - 1] for i in range(len(data_type[0]))]\n",
    "            x2s = [data_batch[data_type[1][i] - 1] for i in range(len(data_type[1]))]\n",
    "            ys = [data_batch[data_type[2][i] - 1] for i in range(len(data_type[2]))]\n",
    "\n",
    "            x1_batch = torch.cat(x1s, dim=1).float().cuda()\n",
    "            x2_batch = torch.cat(x2s, dim=1).float().cuda()\n",
    "            y_batch = torch.cat(ys, dim=1).cuda()\n",
    "\n",
    "            loss, _, _ = model(x1_batch, x2_batch, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            opt_align.step()\n",
    "\n",
    "            # if (_iter + 1) % 1 == 0 and i_batch % 1 == 0:\n",
    "            #     print('iter: ', _iter, ' i_batch: ', i_batch, ' align_loss: ', loss.item())\n",
    "\n",
    "def eval_ce_alignment(model, test_loader, data_type):\n",
    "    results = []\n",
    "    aligns = []\n",
    "\n",
    "    for i_batch, data_batch in enumerate(test_loader):\n",
    "        x1s = [data_batch[data_type[0][i] - 1] for i in range(len(data_type[0]))]\n",
    "        x2s = [data_batch[data_type[1][i] - 1] for i in range(len(data_type[1]))]\n",
    "        ys = [data_batch[data_type[2][i] - 1] for i in range(len(data_type[2]))]\n",
    "\n",
    "        x1_batch = torch.cat(x1s, dim=1).float().cuda()\n",
    "        x2_batch = torch.cat(x2s, dim=1).float().cuda()\n",
    "        y_batch = torch.cat(ys, dim=1).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, result, align = model(x1_batch, x2_batch, y_batch)\n",
    "        results.append(result)\n",
    "        aligns.append(align)\n",
    "\n",
    "    results = torch.stack(results, dim=0)\n",
    " \n",
    "    return results, aligns\n",
    "\n",
    "def critic_ce_alignment(x1, x2, labels, num_labels, train_ds, test_ds, discrim_1=None, discrim_2=None, discrim_12=None, learned_discrim=True, shuffle=True, discrim_epochs=40, ce_epochs=10):\n",
    "    if discrim_1 is not None:\n",
    "        model_discrim_1, model_discrim_2, model_discrim_12 = discrim_1, discrim_2, discrim_12\n",
    "    elif learned_discrim:\n",
    "        model_discrim_1 = Discrim(x_dim=x1.size(1), hidden_dim=32, num_labels=num_labels, layers=3, activation='relu').cuda()\n",
    "        model_discrim_2 = Discrim(x_dim=x2.size(1), hidden_dim=32, num_labels=num_labels, layers=3, activation='relu').cuda()\n",
    "        model_discrim_12 = Discrim(x_dim=x1.size(1) + x2.size(1), hidden_dim=32, num_labels=num_labels, layers=3, activation='relu').cuda()\n",
    "\n",
    "        for model, data_type in [\n",
    "            (model_discrim_1, ([1], [0])),\n",
    "            (model_discrim_2, ([2], [0])),\n",
    "            (model_discrim_12, ([1], [2], [0])),\n",
    "        ]:\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            train_loader1 = DataLoader(train_ds, shuffle=shuffle, drop_last=True,\n",
    "                                    batch_size=batch_size,\n",
    "                                    num_workers=1)\n",
    "            train_discrim(model, train_loader1, optimizer, data_type=data_type, num_epoch=discrim_epochs)\n",
    "            model.eval()\n",
    "            test_loader1 = DataLoader(test_ds, shuffle=False, drop_last=False,\n",
    "                                      batch_size=batch_size, num_workers=1)\n",
    "            eval_discrim(model, test_loader1, data_type=data_type)\n",
    "    else:\n",
    "        model_discrim_1 = simple_discrim([x1], labels, num_labels)\n",
    "        model_discrim_2 = simple_discrim([x2], labels, num_labels)\n",
    "        model_discrim_12 = simple_discrim([x1, x2], labels, num_labels)\n",
    "\n",
    "    p_y = torch.sum(nn.functional.one_hot(labels.squeeze(-1)), dim=0) / len(labels)\n",
    "    # print(p_y)\n",
    "\n",
    "    def product(x):\n",
    "        return x[0] * product(x[1:]) if x else 1\n",
    "\n",
    "    model = CEAlignmentInformation(x1_dim=product(x1.shape[1:]), x2_dim=product(x2.shape[1:]),\n",
    "        hidden_dim=32, embed_dim=10, num_labels=num_labels, layers=3, activation='relu',\n",
    "        discrim_1=model_discrim_1, discrim_2=model_discrim_2, discrim_12=model_discrim_12,\n",
    "        p_y=p_y).cuda()\n",
    "    opt_align = optim.Adam(model.align_parameters(), lr=1e-3)\n",
    "\n",
    "    train_loader1 = DataLoader(train_ds, shuffle=shuffle, drop_last=True,\n",
    "                               batch_size=batch_size,\n",
    "                               num_workers=1)\n",
    "    test_loader1 = DataLoader(test_ds, shuffle=False, drop_last=True,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=1)\n",
    "\n",
    "    # Train and estimate mutual information\n",
    "    model.train()\n",
    "    train_ce_alignment(model, train_loader1, opt_align, data_type=([1], [2], [0]), num_epoch=ce_epochs)\n",
    "\n",
    "    model.eval()\n",
    "    results, aligns = eval_ce_alignment(model, test_loader1, data_type=([1], [2], [0]))\n",
    "    return results, aligns, (model, model_discrim_1, model_discrim_2, model_discrim_12, p_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8AAbS9a0kWz"
   },
   "source": [
    "### Making dataset pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2UPAX8Sm1QJ"
   },
   "outputs": [],
   "source": [
    "traindata, validdata, testdata = get_dataloader('mosei_raw.pkl', modalities=[0,1,2], \n",
    "                                                robust_test=False, max_pad=True, \n",
    "                                                data_type='mosei', max_seq_len=50,\n",
    "                                                batch_size=1058)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbkc8JsVoLij"
   },
   "outputs": [],
   "source": [
    "# humor 18550, 4050, 15000\n",
    "# torch.Size([32, 50, 371])\n",
    "# torch.Size([32, 50, 81])\n",
    "# torch.Size([32, 50, 300])\n",
    "# torch.Size([32, 1])\n",
    "import pickle\n",
    "i = 0\n",
    "x0, x1, x2, lab = None, None, None, None\n",
    "for j in testdata:\n",
    "  if i == 0:\n",
    "    x0, x1, x2, lab = j\n",
    "  else:\n",
    "    x0 = torch.concatenate((x0, j[0]), dim=0)\n",
    "    x1 = torch.concatenate((x1, j[1]), dim=0)\n",
    "    x2 = torch.concatenate((x2, j[2]), dim=0)\n",
    "    lab = torch.concatenate((lab, j[3]), dim=0)\n",
    "  i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vInBu47hvk_q",
    "outputId": "a2b15573-4d24-4f7a-e10a-d7607648be0a"
   },
   "outputs": [],
   "source": [
    "print(x0.shape, x1.shape, x2.shape, lab.shape)\n",
    "N = x0.shape[0]\n",
    "x0 = x0.reshape((N, -1))\n",
    "x1 = x1.reshape((N, -1))\n",
    "x2 = x2.reshape((N, -1))\n",
    "print(x0.shape, x1.shape, x2.shape, lab.shape)\n",
    "\n",
    "data = dict()\n",
    "data['x0'] = x0\n",
    "data['x1'] = x1\n",
    "data['x2'] = x2\n",
    "data['labels'] = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6tnxUO3r_Yd"
   },
   "outputs": [],
   "source": [
    "with open('mosei_test_ce_dataset.pkl', 'wb') as f:\n",
    "  pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUyKc5Os0fqq"
   },
   "source": [
    "### Start of training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7yFcV9E_vpK"
   },
   "outputs": [],
   "source": [
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3YECvbFyFSv"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mosei_valid_ce_dataset.pkl', 'rb') as f:\n",
    "  valid_data = pickle.load(f)\n",
    "with open('mosei_train_ce_dataset.pkl', 'rb') as f:\n",
    "  train_data = pickle.load(f)\n",
    "with open('mosei_test_ce_dataset.pkl', 'rb') as f:\n",
    "  test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSGnKZLB68mo"
   },
   "outputs": [],
   "source": [
    "with open(f'ce_preds/mosei_test_outer_02_pred.pkl', 'rb') as f:\n",
    "  test_pred_labels = pickle.load(f)\n",
    "with open(f'ce_preds/mosei_train_outer_02_pred.pkl', 'rb') as f:\n",
    "  train_pred_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUkcJqW57W11"
   },
   "outputs": [],
   "source": [
    "def flatten(L):\n",
    "  R = L.tolist()\n",
    "  # for p in L:\n",
    "  #   R.extend(list(p.numpy().tolist()))\n",
    "  return [[r] for r in R]\n",
    "test_data['labels'] = (flatten(test_pred_labels))\n",
    "train_data['labels'] = (flatten(train_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7bHcn3G53oC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "replace_neg = np.vectorize (lambda x: 0 if x <= 0 else 1)\n",
    "\n",
    "def get_mm_dataset(modalities, data):\n",
    "  L = []\n",
    "  for mod in modalities:\n",
    "    L.append(data[f'x{mod}'])\n",
    "  labels = replace_neg(data['labels'])\n",
    "  return MultimodalDataset(L, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kts13rmpgVuz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConcatEarly(nn.Module):\n",
    "    \"\"\"Concatenation of input data on dimension 2.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize ConcatEarly Module.\"\"\"\n",
    "        super(ConcatEarly, self).__init__()\n",
    "\n",
    "    def forward(self, modalities):\n",
    "        \"\"\"\n",
    "        Forward Pass of ConcatEarly.\n",
    "        \n",
    "        :param modalities: An iterable of modalities to combine\n",
    "        \"\"\"\n",
    "        return torch.cat(modalities, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-wB9-Q7ofAJ",
    "outputId": "d394aab0-dd11-4614-ca38-7d8e6f3879ca"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from unimodals.common_models import GRU, MLP, Sequential, Identity, Linear  # noqa\n",
    "from training_structures.Supervised_Learning import train, test\n",
    "# from fusions.common_fusions import ConcatEarly  # noqa\n",
    "\n",
    "mod1= 0\n",
    "mod2 = 2\n",
    "for modalities in [[mod1], [mod2], [mod1, mod2]]:\n",
    "  s = ''.join([str(m) for m in modalities])\n",
    "  modelpath = f'ce_preds/mosei_tmp_{s}.pt'\n",
    "  print(modelpath)\n",
    "\n",
    "  valid_ds = get_mm_dataset(modalities, valid_data)\n",
    "  train_ds = get_mm_dataset(modalities, train_data)\n",
    "  test_ds = get_mm_dataset(modalities, test_data)\n",
    "\n",
    "  traindl = DataLoader(train_ds, shuffle=True, num_workers=2, batch_size=32)\n",
    "  valdl = DataLoader(valid_ds, shuffle=False, num_workers=2, batch_size=32)\n",
    "  testdl = DataLoader(test_ds, shuffle=False, num_workers=2, batch_size=32)\n",
    "          \n",
    "\n",
    "  for x in valdl:\n",
    "    for y in x:\n",
    "      print(y.shape)\n",
    "    break\n",
    "\n",
    "  dout = 0\n",
    "  # dims = [18550, 4050, 15000]\n",
    "  dims = [35650, 3700, 15000]\n",
    "  for mod in modalities:\n",
    "    dout += dims[mod]\n",
    "  print(dout)\n",
    "\n",
    "  encoders = [Identity().cuda(),Identity().cuda()]\n",
    "  # head = Sequential(GRU(4050, 1128, dropout=True, has_padding=False, batch_first=True, last_only=True), MLP(1128, 512, 1)).cuda()\n",
    "  # 18550, 4050, 15000\n",
    "  head = Linear(dout, 2).cuda().cuda()\n",
    "\n",
    "  fusion = ConcatEarly().cuda()\n",
    "\n",
    "  train(encoders, fusion, head, traindl, valdl, 10, task=\"classification\", optimtype=torch.optim.AdamW,\n",
    "        is_packed=False, lr=1e-3, save=modelpath, weight_decay=0.01, \n",
    "        objective=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "  print(\"Testing:\", modelpath)\n",
    "  model = torch.load(modelpath).cuda()\n",
    "  test(model, testdl, 'mosei', is_packed=False,\n",
    "      criterion=torch.nn.CrossEntropyLoss(), task=\"classification\", no_robust=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz7zM-cS85pd"
   },
   "outputs": [],
   "source": [
    "mod1 = 0\n",
    "mod2 = 2\n",
    "model1 = torch.load(f'ce_preds/mosei_tmp_{mod1}.pt').cuda()\n",
    "model1.requires_grad = False\n",
    "model1.eval()\n",
    "model2 = torch.load(f'ce_preds/mosei_tmp_{mod2}.pt').cuda()\n",
    "model2.requires_grad = False\n",
    "model2.eval()\n",
    "model12 = torch.load(f'ce_preds/mosei_tmp_{mod1}{mod2}.pt').cuda()\n",
    "model12.requires_grad = False\n",
    "model12.eval()\n",
    "\n",
    "modalities = [mod1,mod2]\n",
    "\n",
    "valid_ds = get_mm_dataset(modalities, valid_data)\n",
    "train_ds = get_mm_dataset(modalities, train_data)\n",
    "test_ds = get_mm_dataset(modalities, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQoREzvP8Qf1",
    "outputId": "37658552-bf4b-42fd-e646-072ac97e99cb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "replace_neg = np.vectorize (lambda x: 0 if x <= 0 else 1)\n",
    "pred = replace_neg(train_data['labels'])\n",
    "# print(pred, len(pred))\n",
    "\n",
    "results = critic_ce_alignment(train_data[f'x{mod1}'], train_data[f'x{mod2}'], torch.tensor(pred), 2, \n",
    "                    train_ds, test_ds, \n",
    "                    discrim_1=model1, discrim_2=model2, discrim_12=model12, \n",
    "                    learned_discrim=True, shuffle=True, discrim_epochs=40, ce_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNiJzcGzLvJW",
    "outputId": "b61e878e-7035-4521-baa4-3baf41c8362c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "res = results[0].cpu().numpy()\n",
    "values = np.mean(res, axis=0)\n",
    "values = values/np.log(2)\n",
    "values = np.maximum(values, 0)\n",
    "print(', '.join([str(v) for v in values]))\n",
    "print(\"Redundancy:\", values[0])\n",
    "print(\"Unique1:\", values[1])\n",
    "print(\"Unique1:\", values[2])\n",
    "print(\"Synergy:\", values[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QeFh4wQIM8p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
